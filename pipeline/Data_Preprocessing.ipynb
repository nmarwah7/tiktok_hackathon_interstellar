{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukCxJ4C912xa"
      },
      "source": [
        "## Overview\n",
        "1. Randomly sample across 3-5 states (e.g., CA, TX, NY, IL, FL) to capture diverse review patterns (urban vs rural, business types, language usage). We sample about 80k-100k non-empty reviews for investigation due to time constraints.\n",
        "\n",
        "2. Sample 500-1000 reviews manually across different states and business types. Aim to balance the noise and non-noise review data for best performance.\n",
        "\n",
        "1. We deployed a light-weight LLM model for pseudo ground-truth labelling.\n",
        "\n",
        "2. Perform resampling on the labelled data to prepare for model training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The step of mounting to google drive could be deleted, or replaced with a step of setting the work directory to where you want."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQYWGG0b_QK3",
        "outputId": "97e20d18-9207-4e34-b4e4-5f8311e77757"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# # Mount Gdrive\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# import os\n",
        "\n",
        "# os.chdir('/content/drive/My Drive/TikTok Hackathon/data gzip')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gm8L0Q9_2eP7"
      },
      "source": [
        "## Data Collection\n",
        "\n",
        "The dataset used for this project is the **Google Local Reviews dataset**. Due to its gigantic volume, we performed random sampling to select a small dataset from it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-_uZ6wLxY3Dp"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5OG2RCn5Xyqf"
      },
      "outputs": [],
      "source": [
        "meta_California = 'meta-California.json.gz'\n",
        "meta_Illinois = 'meta-Illinois.json.gz'\n",
        "meta_Louisana = 'meta-Louisiana.json.gz'\n",
        "meta_Texas = 'meta-Texas.json.gz'\n",
        "meta_NewYork = 'meta-New_York.json.gz'\n",
        "\n",
        "review_California = 'review-California_10.json.gz'\n",
        "review_Illinois = 'review-Illinois_10.json.gz'\n",
        "review_Louisana = 'review-Louisiana_10.json.gz'\n",
        "review_NewYork = 'review-New_York_10.json.gz'\n",
        "review_Texas = 'review-Texas_10.json.gz'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yoPCRTrsY_k_"
      },
      "source": [
        "Test examine New York data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rswFQiwYXwU4"
      },
      "outputs": [],
      "source": [
        "df = pd.read_json(meta_NewYork, compression='gzip', lines=True)\n",
        "\n",
        "# Preview the first few rows\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4e8vUUNi1dYJ"
      },
      "source": [
        "We randomly sample 0.1% from the entire dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xeUq7sqjwIz3"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import random\n",
        "\n",
        "# Input / output files\n",
        "input_file = \"dataset.json\" #modify path\n",
        "output_file = \"sampled_dataset.json\"\n",
        "\n",
        "# Load JSON data\n",
        "with open(input_file, \"r\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Make sure data is a list\n",
        "if not isinstance(data, list):\n",
        "    raise ValueError(\"JSON root must be a list of records.\")\n",
        "\n",
        "# Calculate sample size (at least 1 if dataset is small)\n",
        "sample_size = max(1, int(0.001 * len(data)))\n",
        "\n",
        "# Randomly sample 1%\n",
        "sampled_data = random.sample(data, sample_size)\n",
        "\n",
        "# Save to new JSON file\n",
        "with open(output_file, \"w\") as f:\n",
        "    json.dump(sampled_data, f, indent=2)\n",
        "\n",
        "print(f\"Sampled {sample_size} records out of {len(data)} into {output_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ey5Jvy2-uX4Z"
      },
      "source": [
        "### Util: gzip reader in chunk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HQDPyOLUud4_"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import gzip\n",
        "import json\n",
        "\n",
        "def gz_to_df(input_path):\n",
        "    if input_path.endswith('.json.gz'):\n",
        "        with gzip.open(input_path, 'rt', encoding='utf-8') as f:\n",
        "            data = f.readlines()\n",
        "\n",
        "    data = map(lambda x: x.rstrip(), data)\n",
        "    data_json_str = \"[\" + ','.join(data) + \"]\"\n",
        "    data_df = pd.read_json(data_json_str)\n",
        "    return data_df\n",
        "\n",
        "def gz_to_df_chunks(input_path, chunk_size=100, threshold=5000):\n",
        "    print(\"reading json in chunks\")\n",
        "    if not input_path.endswith('.json.gz'):\n",
        "        raise ValueError(\"Input file must be a .json.gz file\")\n",
        "\n",
        "    records = []\n",
        "    num_chunks = 0\n",
        "    with gzip.open(input_path, 'rt', encoding='utf-8') as f:\n",
        "        for i, line in enumerate(f):\n",
        "            if num_chunks > threshold/chunk_size:\n",
        "              break\n",
        "            records.append(json.loads(line))\n",
        "            if (i + 1) % chunk_size == 0:\n",
        "                print(f\"producing chunk {int((i + 1) / chunk_size)}\")\n",
        "                yield pd.DataFrame(records)\n",
        "                num_chunks += 1\n",
        "                records = []\n",
        "        # Yield any remaining records as the last chunk\n",
        "        if records:\n",
        "            print(f\"producing final chunk\")\n",
        "            yield pd.DataFrame(records)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9iwwMpjG-F1"
      },
      "source": [
        "## Data cleaning and combining\n",
        "Combining review data and metadata, joining on `gmap_id`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZUaEuPZHArr"
      },
      "outputs": [],
      "source": [
        "def clean_combine_chunk(review_df, meta_df):\n",
        "    columns_to_keep = ['gmap_id', 'name', 'category', 'description', 'avg_rating']\n",
        "    cleaned_meta = meta_df[columns_to_keep].drop_duplicates(subset=\"gmap_id\", keep=\"first\")\n",
        "    columns_to_keep = ['rating', 'text', 'gmap_id']\n",
        "    # Drop rows with null or empty or blank 'text' field\n",
        "    cleaned_review = review_df[review_df['text'].notnull() & (review_df['text'].str.strip() != '')]\n",
        "    print(f\"number of non-empty reviews:\", {len(cleaned_review)})\n",
        "    cleaned_review = cleaned_review.loc[:, columns_to_keep]\n",
        "    print(f\"cleaned review size: {len(cleaned_review)}\")\n",
        "    # remove new line symboles\n",
        "    cleaned_review[\"text\"] = cleaned_review[\"text\"].str.replace(r\"[\\r\\n]+\", \" \", regex=True)\n",
        "    # Extract translated text\n",
        "    pattern = r\"\\(Translated by Google\\) (.*?) \\(Original\\)\"\n",
        "    cleaned_review[\"text\"] = cleaned_review[\"text\"].str.extract(pattern)[0].fillna(cleaned_review[\"text\"])\n",
        "\n",
        "    combined_df = pd.merge(cleaned_review, cleaned_meta, on='gmap_id', how='inner')\n",
        "    print(f\"combined df size: {len(combined_df)}\")\n",
        "    return combined_df\n",
        "\n",
        "def clean_chunk(review_df):\n",
        "  columns_to_keep = ['rating', 'text', 'gmap_id']\n",
        "  # Drop rows with null or empty or blank 'text' field\n",
        "  cleaned_review = review_df[review_df['text'].notnull() & (review_df['text'].str.strip() != '')]\n",
        "  print(f\"number of non-empty reviews:\", {len(cleaned_review)})\n",
        "  cleaned_review = cleaned_review.loc[:, columns_to_keep]\n",
        "  return cleaned_review"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mvIJ531-wrmr",
        "outputId": "f60443d4-fae0-4f4a-ac29-9796f3250c6c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-2374595152.py:12: FutureWarning: Passing literal json to 'read_json' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
            "  data_df = pd.read_json(data_json_str)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "reading json in chunks\n",
            "producing chunk 1\n",
            "Processing chunk 1\n",
            "number of non-empty reviews: {10}\n",
            "cleaned review size: 10\n",
            "combined df size: 10\n",
            "Chunk after:    rating                                               text  \\\n",
            "0       5  I'm late to posting this but this store especi...   \n",
            "1       1  Very dissatisfied I did not get my phone the p...   \n",
            "2       5  Excellent very well done with professional car...   \n",
            "3       5  Basing my review strictly on the service I rec...   \n",
            "4       1  Bad! Disorganized. I'm being totally honest. I...   \n",
            "5       1  Worse customer ever ! More then 30min to make ...   \n",
            "6       5  Excellent very well done with professional car...   \n",
            "7       1  Worse customer ever ! More then 30min to make ...   \n",
            "8       5                                    Very good store   \n",
            "9       5  Thank you, prompt and knowledgeable attention....   \n",
            "\n",
            "                               gmap_id      name  \\\n",
            "0  0x89c25fc9494dce47:0x6d63c807b59a55  T-Mobile   \n",
            "1  0x89c25fc9494dce47:0x6d63c807b59a55  T-Mobile   \n",
            "2  0x89c25fc9494dce47:0x6d63c807b59a55  T-Mobile   \n",
            "3  0x89c25fc9494dce47:0x6d63c807b59a55  T-Mobile   \n",
            "4  0x89c25fc9494dce47:0x6d63c807b59a55  T-Mobile   \n",
            "5  0x89c25fc9494dce47:0x6d63c807b59a55  T-Mobile   \n",
            "6  0x89c25fc9494dce47:0x6d63c807b59a55  T-Mobile   \n",
            "7  0x89c25fc9494dce47:0x6d63c807b59a55  T-Mobile   \n",
            "8  0x89c25fc9494dce47:0x6d63c807b59a55  T-Mobile   \n",
            "9  0x89c25fc9494dce47:0x6d63c807b59a55  T-Mobile   \n",
            "\n",
            "                                            category description  avg_rating  \n",
            "0  [Cell phone store, Electronic parts supplier, ...        None         3.5  \n",
            "1  [Cell phone store, Electronic parts supplier, ...        None         3.5  \n",
            "2  [Cell phone store, Electronic parts supplier, ...        None         3.5  \n",
            "3  [Cell phone store, Electronic parts supplier, ...        None         3.5  \n",
            "4  [Cell phone store, Electronic parts supplier, ...        None         3.5  \n",
            "5  [Cell phone store, Electronic parts supplier, ...        None         3.5  \n",
            "6  [Cell phone store, Electronic parts supplier, ...        None         3.5  \n",
            "7  [Cell phone store, Electronic parts supplier, ...        None         3.5  \n",
            "8  [Cell phone store, Electronic parts supplier, ...        None         3.5  \n",
            "9  [Cell phone store, Electronic parts supplier, ...        None         3.5  \n"
          ]
        }
      ],
      "source": [
        "meta_df = gz_to_df(meta_NewYork)\n",
        "chunks = gz_to_df_chunks(review_NewYork, chunk_size=10, threshold=1)\n",
        "for i, chunk in enumerate(chunks):\n",
        "    print(f\"Processing chunk {i+1}\")\n",
        "    cleaned_chunk = clean_combine_chunk(chunk, meta_df)\n",
        "    print(f\"Chunk after: {cleaned_chunk}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8nOMBirujCK"
      },
      "source": [
        "## Pseudo Ground-truth Labelling\n",
        "Perform zero-shot-classification on batched sample data, detecting noise reviews."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XP85wt5ZRWmh"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install pyarrow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lJ9AkEonSO5w"
      },
      "outputs": [],
      "source": [
        "!pip install -U transformers torch accelerate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A52bqqr93iBT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from transformers import pipeline\n",
        "import signal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yXLzyN23FleQ"
      },
      "outputs": [],
      "source": [
        "relevant = 0\n",
        "ads = 0\n",
        "no_experience = 0\n",
        "irrelevant = 0\n",
        "\n",
        "def label_chunk(df_chunk, classifier, candidate_labels, threshold):\n",
        "  global relevant, ads, no_experience, irrelevant\n",
        "  print(f\"processing batch\")\n",
        "  predictions = []\n",
        "\n",
        "  for _, row in chunk.iterrows():\n",
        "      # Combine text + rating for model context\n",
        "      input_text = f\"Business Description: {row['name']}, {row['category']}, {row['description']}, average rating = {row['avg_rating']}. User's review: {row['text']}, rating = {row['rating']}/5.\"\n",
        "      result = classifier(input_text, candidate_labels=labels)\n",
        "      # If the score for selected label is above threshold, keep it in the final predictions\n",
        "      if result[\"scores\"][0] > threshold:\n",
        "          if result[\"labels\"][0] == \"relevant review\":\n",
        "              relevant += 1\n",
        "          elif result[\"labels\"][0] == \"advertisement / spam\":\n",
        "              ads += 1\n",
        "          elif result[\"labels\"][0] == \"no first-hand experience\":\n",
        "              no_experience += 1\n",
        "              # If the score for selected label is above threshold, keep it in the final predictions\n",
        "          elif result[\"labels\"][0] == \"irrelevant / off-topic\":\n",
        "              irrelevant += 1\n",
        "          pass\n",
        "          # Put the index of the row\n",
        "          predictions.append({\n",
        "              \"text\": row[\"text\"],\n",
        "              \"rating\": row[\"rating\"],\n",
        "              \"category\": result[\"labels\"][0],\n",
        "              \"score\": result[\"scores\"][0],\n",
        "              \"label\": \"relevant\" if result[\"labels\"][0] == \"relevant review\" else \"irrelevant\"\n",
        "          })\n",
        "  return predictions\n",
        "\n",
        "def label_chunk_naiive(df_chunk, classifier, candidate_labels, threshold):\n",
        "  global relevant, ads, no_experience, irrelevant\n",
        "  print(f\"processing batch\")\n",
        "  predictions = []\n",
        "\n",
        "  for _, row in chunk.iterrows():\n",
        "      # Combine text + rating for model context\n",
        "      input_text = f\"User's review: {row['text']}, rating = {row['rating']}/5.\"\n",
        "      result = classifier(input_text, candidate_labels=labels)\n",
        "      # If the score for selected label is above threshold, keep it in the final predictions\n",
        "      if result[\"scores\"][0] > threshold:\n",
        "          if result[\"labels\"][0] == \"relevant review\":\n",
        "              relevant += 1\n",
        "          elif result[\"labels\"][0] == \"advertisement / spam\":\n",
        "              ads += 1\n",
        "          elif result[\"labels\"][0] == \"no first-hand experience\":\n",
        "              no_experience += 1\n",
        "              # If the score for selected label is above threshold, keep it in the final predictions\n",
        "          elif result[\"labels\"][0] == \"irrelevant / off-topic\":\n",
        "              irrelevant += 1\n",
        "          pass\n",
        "          # Put the index of the row\n",
        "          predictions.append({\n",
        "              \"text\": row[\"text\"],\n",
        "              \"rating\": row[\"rating\"],\n",
        "              \"category\": result[\"labels\"][0],\n",
        "              \"score\": result[\"scores\"][0],\n",
        "              \"label\": \"relevant\" if result[\"labels\"][0] == \"relevant review\" else \"irrelevant\"\n",
        "          })\n",
        "  return predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tqj2815o8thC",
        "outputId": "b5ce8517-25bc-4b63-ba35-9889e178619f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loading df\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-2374595152.py:12: FutureWarning: Passing literal json to 'read_json' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
            "  data_df = pd.read_json(data_json_str)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "initializing classifier\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ],
      "source": [
        "# Open a parquet file to store results\n",
        "results_file = \"labeled_reviews.parquet\"\n",
        "results_df = pd.DataFrame()\n",
        "\n",
        "chunk_size = 100\n",
        "threshold_total = 10  # Total number of reviews to process\n",
        "\n",
        "print(\"loading df\")\n",
        "meta_df = gz_to_df(meta_NewYork)\n",
        "chunks = gz_to_df_chunks(review_NewYork, chunk_size=chunk_size)\n",
        "\n",
        "print(\"initializing classifier\")\n",
        "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
        "labels = [\"relevant review\", \"advertisement / spam\", \"no first-hand experience\", \"irrelevant / off-topic\"]\n",
        "\n",
        "all_results = []\n",
        "threshold = 0.5\n",
        "relevant = 0\n",
        "ads = 0\n",
        "no_experience = 0\n",
        "irrelevant = 0\n",
        "\n",
        "stop_requested = False\n",
        "\n",
        "def handle_interrupt(signum, frame):\n",
        "    global stop_requested\n",
        "    print(\"\\nManual stop requested. Will finish current chunk and exit gracefully.\")\n",
        "    stop_requested = True\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9K_yrYue5aNU"
      },
      "source": [
        "### For running cleaned combined data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eW0xmmq2qf6O"
      },
      "outputs": [],
      "source": [
        "signal.signal(signal.SIGINT, handle_interrupt)\n",
        "\n",
        "for i, chunk in enumerate(chunks):\n",
        "    print(f\"Processing chunk {i+1}\")\n",
        "    chunk = clean_combine_chunk(chunk, meta_df)\n",
        "    print(f\"Chunk size after cleaning: {len(chunk)}\")\n",
        "    if not chunk.empty:\n",
        "        predictions = label_chunk(chunk, classifier, labels, threshold)\n",
        "        all_results.extend(predictions)\n",
        "    if stop_requested:\n",
        "        print(\"Stopping after current chunk.\")\n",
        "        break\n",
        "\n",
        "# Convert results to DataFrame\n",
        "results_df = pd.DataFrame(all_results)\n",
        "# Save results to parquet file\n",
        "results_df.to_parquet(results_file, index=False)\n",
        "# Print number of each label\n",
        "print(results_df['category'].value_counts())\n",
        "\n",
        "for label in results_df['category'].unique():\n",
        "    print(f\"\\nLabel: {label}\")\n",
        "    print(results_df[results_df['category'] == label].head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WiL1cZGV8EGm"
      },
      "source": [
        "### For simple labeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h4CrrxpB8F8Y",
        "outputId": "7b7b3114-855b-405c-d7e3-7ca3466dc9a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "reading json in chunks\n",
            "producing chunk 1\n",
            "Processing chunk 1\n",
            "number of non-empty reviews: {51}\n",
            "Chunk size after cleaning: 51\n",
            "processing batch\n",
            "relevant: 49, ads: 0, no_experience: 1, irrelevant: 1\n",
            "producing chunk 2\n",
            "Processing chunk 2\n",
            "number of non-empty reviews: {75}\n",
            "Chunk size after cleaning: 75\n",
            "processing batch\n",
            "relevant: 123, ads: 0, no_experience: 2, irrelevant: 1\n",
            "producing chunk 3\n",
            "Processing chunk 3\n",
            "number of non-empty reviews: {59}\n",
            "Chunk size after cleaning: 59\n",
            "processing batch\n",
            "relevant: 182, ads: 0, no_experience: 2, irrelevant: 1\n",
            "producing chunk 4\n",
            "Processing chunk 4\n",
            "number of non-empty reviews: {56}\n",
            "Chunk size after cleaning: 56\n",
            "processing batch\n",
            "relevant: 238, ads: 0, no_experience: 2, irrelevant: 1\n",
            "producing chunk 5\n",
            "Processing chunk 5\n",
            "number of non-empty reviews: {62}\n",
            "Chunk size after cleaning: 62\n",
            "processing batch\n",
            "relevant: 297, ads: 0, no_experience: 4, irrelevant: 1\n",
            "producing chunk 6\n",
            "Processing chunk 6\n",
            "number of non-empty reviews: {55}\n",
            "Chunk size after cleaning: 55\n",
            "processing batch\n",
            "relevant: 351, ads: 0, no_experience: 5, irrelevant: 1\n",
            "producing chunk 7\n",
            "Processing chunk 7\n",
            "number of non-empty reviews: {67}\n",
            "Chunk size after cleaning: 67\n",
            "processing batch\n",
            "relevant: 417, ads: 0, no_experience: 6, irrelevant: 1\n",
            "producing chunk 8\n",
            "Processing chunk 8\n",
            "number of non-empty reviews: {75}\n",
            "Chunk size after cleaning: 75\n",
            "processing batch\n",
            "relevant: 490, ads: 0, no_experience: 7, irrelevant: 1\n",
            "producing chunk 9\n",
            "Processing chunk 9\n",
            "number of non-empty reviews: {60}\n",
            "Chunk size after cleaning: 60\n",
            "processing batch\n",
            "relevant: 550, ads: 0, no_experience: 7, irrelevant: 1\n",
            "producing chunk 10\n",
            "Processing chunk 10\n",
            "number of non-empty reviews: {63}\n",
            "Chunk size after cleaning: 63\n",
            "processing batch\n",
            "relevant: 613, ads: 0, no_experience: 7, irrelevant: 1\n",
            "producing chunk 11\n",
            "Processing chunk 11\n",
            "number of non-empty reviews: {65}\n",
            "Chunk size after cleaning: 65\n",
            "processing batch\n",
            "relevant: 678, ads: 0, no_experience: 7, irrelevant: 1\n",
            "producing chunk 12\n",
            "Processing chunk 12\n",
            "number of non-empty reviews: {67}\n",
            "Chunk size after cleaning: 67\n",
            "processing batch\n",
            "relevant: 742, ads: 0, no_experience: 8, irrelevant: 2\n",
            "producing chunk 13\n",
            "Processing chunk 13\n",
            "number of non-empty reviews: {66}\n",
            "Chunk size after cleaning: 66\n",
            "processing batch\n",
            "relevant: 807, ads: 0, no_experience: 9, irrelevant: 2\n",
            "producing chunk 14\n",
            "Processing chunk 14\n",
            "number of non-empty reviews: {60}\n",
            "Chunk size after cleaning: 60\n",
            "processing batch\n",
            "relevant: 867, ads: 0, no_experience: 9, irrelevant: 2\n",
            "producing chunk 15\n",
            "Processing chunk 15\n",
            "number of non-empty reviews: {64}\n",
            "Chunk size after cleaning: 64\n",
            "processing batch\n",
            "relevant: 931, ads: 0, no_experience: 9, irrelevant: 2\n",
            "producing chunk 16\n",
            "Processing chunk 16\n",
            "number of non-empty reviews: {76}\n",
            "Chunk size after cleaning: 76\n",
            "processing batch\n",
            "relevant: 1005, ads: 0, no_experience: 10, irrelevant: 2\n",
            "producing chunk 17\n",
            "Processing chunk 17\n",
            "number of non-empty reviews: {63}\n",
            "Chunk size after cleaning: 63\n",
            "processing batch\n",
            "relevant: 1067, ads: 0, no_experience: 11, irrelevant: 2\n",
            "producing chunk 18\n",
            "Processing chunk 18\n",
            "number of non-empty reviews: {60}\n",
            "Chunk size after cleaning: 60\n",
            "processing batch\n",
            "relevant: 1126, ads: 0, no_experience: 12, irrelevant: 2\n",
            "producing chunk 19\n",
            "Processing chunk 19\n",
            "number of non-empty reviews: {66}\n",
            "Chunk size after cleaning: 66\n",
            "processing batch\n",
            "relevant: 1192, ads: 0, no_experience: 12, irrelevant: 2\n",
            "producing chunk 20\n",
            "Processing chunk 20\n",
            "number of non-empty reviews: {71}\n",
            "Chunk size after cleaning: 71\n",
            "processing batch\n",
            "relevant: 1263, ads: 0, no_experience: 12, irrelevant: 2\n",
            "producing chunk 21\n",
            "Processing chunk 21\n",
            "number of non-empty reviews: {76}\n",
            "Chunk size after cleaning: 76\n",
            "processing batch\n",
            "relevant: 1339, ads: 0, no_experience: 12, irrelevant: 2\n",
            "producing chunk 22\n",
            "Processing chunk 22\n",
            "number of non-empty reviews: {48}\n",
            "Chunk size after cleaning: 48\n",
            "processing batch\n",
            "relevant: 1387, ads: 0, no_experience: 12, irrelevant: 2\n",
            "producing chunk 23\n",
            "Processing chunk 23\n",
            "number of non-empty reviews: {58}\n",
            "Chunk size after cleaning: 58\n",
            "processing batch\n",
            "relevant: 1445, ads: 0, no_experience: 12, irrelevant: 2\n",
            "producing chunk 24\n",
            "Processing chunk 24\n",
            "number of non-empty reviews: {60}\n",
            "Chunk size after cleaning: 60\n",
            "processing batch\n",
            "relevant: 1505, ads: 0, no_experience: 12, irrelevant: 2\n",
            "producing chunk 25\n",
            "Processing chunk 25\n",
            "number of non-empty reviews: {54}\n",
            "Chunk size after cleaning: 54\n",
            "processing batch\n",
            "relevant: 1558, ads: 0, no_experience: 13, irrelevant: 2\n",
            "producing chunk 26\n",
            "Processing chunk 26\n",
            "number of non-empty reviews: {71}\n",
            "Chunk size after cleaning: 71\n",
            "processing batch\n",
            "\n",
            "Manual stop requested. Will finish current chunk and exit gracefully.\n",
            "relevant: 1627, ads: 0, no_experience: 14, irrelevant: 2\n",
            "Stopping after current chunk.\n",
            "category\n",
            "relevant review             1627\n",
            "no first-hand experience      14\n",
            "irrelevant / off-topic         2\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Label: relevant review\n",
            "                                                text  rating         category  \\\n",
            "0  I'm late to posting this but this store especi...       5  relevant review   \n",
            "1  Very dissatisfied I did not get my phone the p...       1  relevant review   \n",
            "2  Excellent very well done with professional car...       5  relevant review   \n",
            "3  Basing my review strictly on the service I rec...       5  relevant review   \n",
            "4  Bad! Disorganized. I'm being totally honest. I...       1  relevant review   \n",
            "5  Worse customer ever ! More then 30min to make ...       1  relevant review   \n",
            "6  Excellent very well done with professional car...       5  relevant review   \n",
            "7  Worse customer ever ! More then 30min to make ...       1  relevant review   \n",
            "8  (Translated by Google) Very good store\\n\\n(Ori...       5  relevant review   \n",
            "9  (Translated by Google) Thank you, prompt and k...       5  relevant review   \n",
            "\n",
            "      score     label  \n",
            "0  0.929778  relevant  \n",
            "1  0.925132  relevant  \n",
            "2  0.965150  relevant  \n",
            "3  0.967948  relevant  \n",
            "4  0.953626  relevant  \n",
            "5  0.949401  relevant  \n",
            "6  0.965150  relevant  \n",
            "7  0.949401  relevant  \n",
            "8  0.911436  relevant  \n",
            "9  0.906798  relevant  \n",
            "\n",
            "Label: no first-hand experience\n",
            "                                                  text  rating  \\\n",
            "28   Great place has never been here but it's wonde...       5   \n",
            "117  This is my bottle return place. Judy is my gir...       5   \n",
            "275                                       No play area       3   \n",
            "282                                NEVER HEARD OF THEM       1   \n",
            "312                                  Haven't been here       1   \n",
            "410  Can't say much about the place as I only use o...       5   \n",
            "477  Perfect for me.  ( Just an opinion, because yo...       5   \n",
            "739                                          Dont know       3   \n",
            "782                      Only looked it's a nice store       3   \n",
            "990                          Change store hours online       1   \n",
            "\n",
            "                     category     score       label  \n",
            "28   no first-hand experience  0.885968  irrelevant  \n",
            "117  no first-hand experience  0.708865  irrelevant  \n",
            "275  no first-hand experience  0.616911  irrelevant  \n",
            "282  no first-hand experience  0.918668  irrelevant  \n",
            "312  no first-hand experience  0.955883  irrelevant  \n",
            "410  no first-hand experience  0.847698  irrelevant  \n",
            "477  no first-hand experience  0.778574  irrelevant  \n",
            "739  no first-hand experience  0.866195  irrelevant  \n",
            "782  no first-hand experience  0.877860  irrelevant  \n",
            "990  no first-hand experience  0.725221  irrelevant  \n",
            "\n",
            "Label: irrelevant / off-topic\n",
            "                             text  rating                category     score  \\\n",
            "45   Not really much to see here.       3  irrelevant / off-topic  0.514006   \n",
            "701            This area is junky       3  irrelevant / off-topic  0.799172   \n",
            "\n",
            "          label  \n",
            "45   irrelevant  \n",
            "701  irrelevant  \n"
          ]
        }
      ],
      "source": [
        "signal.signal(signal.SIGINT, handle_interrupt)\n",
        "\n",
        "for i, chunk in enumerate(chunks):\n",
        "    print(f\"Processing chunk {i+1}\")\n",
        "    chunk = clean_chunk(chunk)\n",
        "    print(f\"Chunk size after cleaning: {len(chunk)}\")\n",
        "    if not chunk.empty:\n",
        "        predictions = label_chunk_naiive(chunk, classifier, labels, threshold)\n",
        "        all_results.extend(predictions)\n",
        "        print(f\"relevant: {relevant}, ads: {ads}, no_experience: {no_experience}, irrelevant: {irrelevant}\")\n",
        "    if stop_requested:\n",
        "        print(\"Stopping after current chunk.\")\n",
        "        break\n",
        "\n",
        "# Convert results to DataFrame\n",
        "results_df = pd.DataFrame(all_results)\n",
        "# Save results to parquet file\n",
        "results_df.to_parquet(results_file, index=False)\n",
        "# Print number of each label\n",
        "print(results_df['category'].value_counts())\n",
        "\n",
        "for label in results_df['category'].unique():\n",
        "    print(f\"\\nLabel: {label}\")\n",
        "    print(results_df[results_df['category'] == label].head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-f8EmV-ujH3"
      },
      "source": [
        "### Import synthetic data for ads/spam and irrelevant/off-topic reviews\n",
        "\n",
        "There are very little noise reviews in the dataset. We therefore generate some synthetic data to balance the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iw2DhjcUuih2",
        "outputId": "8397975a-b8c0-45ab-af58-6bb044c0fd52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                text  category\n",
            "0  Get 50% off designer sunglasses today! Visit w...  ads_spam\n",
            "1  Earn $1000 per day working from home. Click he...  ads_spam\n",
            "2  Congratulations! You’ve been selected for a fr...  ads_spam\n",
            "3  Buy followers instantly and grow your profile ...  ads_spam\n",
            "4  Lose weight in 2 weeks with this miracle pill....  ads_spam\n",
            "\n",
            "Total rows: 60\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "ads_spam = [\n",
        "    \"Get 50% off designer sunglasses today! Visit www.discountsun.com now!\",\n",
        "    \"Earn $1000 per day working from home. Click here for details.\",\n",
        "    \"Congratulations! You’ve been selected for a free cruise. Claim now.\",\n",
        "    \"Buy followers instantly and grow your profile fast.\",\n",
        "    \"Lose weight in 2 weeks with this miracle pill. Order now!\",\n",
        "    \"Click here to claim your free iPhone today!\",\n",
        "    \"Don’t miss this investment opportunity. Double your money fast.\",\n",
        "    \"Sign up for unlimited movie streaming. First month free!\",\n",
        "    \"Exclusive deal! Buy one get one free at our online store.\",\n",
        "    \"Visit www.hotdeals.com for daily discounts you can’t miss.\",\n",
        "    \"Win big in our lottery. Just send your email to enter.\",\n",
        "    \"Cheap prescription meds without a doctor visit. Shop now!\",\n",
        "    \"Guaranteed credit approval. Apply online in 2 minutes!\",\n",
        "    \"Limited time offer: free shipping on all products.\",\n",
        "    \"Download this app to earn rewards instantly.\",\n",
        "    \"Claim your cash prize now. Limited spots left!\",\n",
        "    \"Upgrade your internet speed for just $9.99/month.\",\n",
        "    \"Get rich quick with this proven business system.\",\n",
        "    \"Free membership to our dating site for 7 days.\",\n",
        "    \"Save 70% on electronics. Today only!\",\n",
        "    \"Want clear skin? Try this new cream now.\",\n",
        "    \"Hot singles in your area are waiting to chat.\",\n",
        "    \"Protect your computer with our free antivirus download.\",\n",
        "    \"Join our VIP club and get instant perks.\",\n",
        "    \"Affordable travel packages available now. Book today!\",\n",
        "    \"Work from anywhere and earn six figures annually.\",\n",
        "    \"Claim your gift card before it expires.\",\n",
        "    \"Unlock premium features with our special promo code.\",\n",
        "    \"Congratulations! You are our lucky winner.\",\n",
        "    \"Hurry, stocks running out fast. Buy before it’s gone!\"\n",
        "]\n",
        "\n",
        "irrelevant = [\n",
        "    \"I just bought a new vacuum cleaner and it works great.\",\n",
        "    \"The weather here has been crazy hot lately.\",\n",
        "    \"I’m so excited for the next Marvel movie release.\",\n",
        "    \"This phone case I ordered online is super durable.\",\n",
        "    \"I’ve been baking bread every weekend—so relaxing.\",\n",
        "    \"My dog learned a new trick yesterday!\",\n",
        "    \"I stayed up late watching soccer last night.\",\n",
        "    \"The traffic in my city is unbearable during rush hour.\",\n",
        "    \"I think pineapple on pizza actually tastes amazing.\",\n",
        "    \"I need recommendations for good sci-fi books.\",\n",
        "    \"Just finished painting my living room blue.\",\n",
        "    \"Does anyone else still play old Nintendo games?\",\n",
        "    \"I went hiking in the mountains last weekend.\",\n",
        "    \"Currently obsessed with this new coffee blend.\",\n",
        "    \"I can’t believe how expensive gas has become.\",\n",
        "    \"Got my car washed today and it looks brand new.\",\n",
        "    \"Thinking of adopting another cat soon.\",\n",
        "    \"This online class I’m taking is super helpful.\",\n",
        "    \"I want to learn how to play the guitar.\",\n",
        "    \"Been experimenting with photography—so fun!\",\n",
        "    \"I’m rewatching my favorite TV series from childhood.\",\n",
        "    \"Started going to the gym again after months.\",\n",
        "    \"My laptop battery drains way too quickly.\",\n",
        "    \"I’ve been gardening a lot this spring.\",\n",
        "    \"My kid just started kindergarten today.\",\n",
        "    \"I’m craving sushi right now.\",\n",
        "    \"Learning a new language is harder than I expected.\",\n",
        "    \"I can’t wait for my vacation next month.\",\n",
        "    \"Been binge-watching documentaries on space.\",\n",
        "    \"Trying out a new skincare routine this week.\"\n",
        "]\n",
        "\n",
        "# Build dataset\n",
        "data = []\n",
        "for text in ads_spam:\n",
        "    data.append({\"text\": text, \"category\": \"advertisement / spam\"})\n",
        "for text in irrelevant:\n",
        "    data.append({\"text\": text, \"category\": \"irrelevant / off-topic\"})\n",
        "\n",
        "synthetic_df = pd.DataFrame(data)\n",
        "synthetic_df[\"rating\"] = np.random.randint(1, 6, size=len(df))\n",
        "synthetic_df[\"label\"] = \"irrelevant\"\n",
        "\n",
        "print(df.head())\n",
        "print(f\"\\nTotal rows: {len(df)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WurfjJwDwQ0A"
      },
      "outputs": [],
      "source": [
        "synthetic_df.to_parquet(\"synthetic_ads_irrelevant.parquet\", index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "ukCxJ4C912xa",
        "gm8L0Q9_2eP7"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
